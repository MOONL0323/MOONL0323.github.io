{"meta":{"title":"Unknown Moon","subtitle":"","description":"记录一些关于编程的思考","author":"MOONL(林凯圳)","url":"http://MOONL0323.github.io","root":"/"},"pages":[{"title":"所有分类","date":"2025-02-04T07:16:58.969Z","updated":"2025-02-04T07:16:58.969Z","comments":true,"path":"categories/index.html","permalink":"http://moonl0323.github.io/categories/index.html","excerpt":"","text":""},{"title":"我的朋友们","date":"2025-02-04T06:33:17.236Z","updated":"2025-02-04T06:33:17.236Z","comments":true,"path":"friends/index.html","permalink":"http://moonl0323.github.io/friends/index.html","excerpt":"这里写友链上方的内容。","text":"这里写友链上方的内容。 这里可以写友链页面下方的文字备注，例如自己的友链规范、示例等。"},{"title":"","date":"2025-02-04T06:31:51.567Z","updated":"2025-02-04T06:31:51.567Z","comments":true,"path":"about/index.html","permalink":"http://moonl0323.github.io/about/index.html","excerpt":"","text":"下面写关于自己的内容"},{"title":"所有标签","date":"2025-02-04T06:32:39.207Z","updated":"2025-02-04T06:32:39.207Z","comments":true,"path":"tags/index.html","permalink":"http://moonl0323.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"亿级流量系统架构设计与实践勘误","slug":"亿级流量系统架构设计与实践勘误","date":"2024-11-30T16:00:00.000Z","updated":"2025-02-04T07:00:09.661Z","comments":true,"path":"2024/12/01/亿级流量系统架构设计与实践勘误/","permalink":"http://moonl0323.github.io/2024/12/01/%E4%BA%BF%E7%BA%A7%E6%B5%81%E9%87%8F%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E8%B7%B5%E5%8B%98%E8%AF%AF/","excerpt":"","text":"亿级流量系统架构设计与实践勘误 p256 图7-4 应该是审核号大于线上版本号，这里反了，图的逻辑有问题 p317 覆盖索引那里，应该是”空间换时间“而不是”时间换空间“","categories":[{"name":"书籍阅读","slug":"书籍阅读","permalink":"http://moonl0323.github.io/categories/%E4%B9%A6%E7%B1%8D%E9%98%85%E8%AF%BB/"},{"name":"后端开发","slug":"书籍阅读/后端开发","permalink":"http://moonl0323.github.io/categories/%E4%B9%A6%E7%B1%8D%E9%98%85%E8%AF%BB/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"后端开发","slug":"后端开发","permalink":"http://moonl0323.github.io/tags/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"},{"name":"架构设计","slug":"架构设计","permalink":"http://moonl0323.github.io/tags/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"}]},{"title":"内容发布系统实际开发","slug":"内容发布系统实际开发","date":"2024-11-30T16:00:00.000Z","updated":"2025-02-04T06:59:33.007Z","comments":true,"path":"2024/12/01/内容发布系统实际开发/","permalink":"http://moonl0323.github.io/2024/12/01/%E5%86%85%E5%AE%B9%E5%8F%91%E5%B8%83%E7%B3%BB%E7%BB%9F%E5%AE%9E%E9%99%85%E5%BC%80%E5%8F%91/","excerpt":"","text":"内容发布系统实际开发首先建表： 1234567891011121314151617181920212223242526CREATE TABLE item_info ( item_id BIGINT PRIMARY KEY, creator_id BIGINT, online_version INT, online_image_uris VARCHAR(255), online_video_id BIGINT, online_text_uri VARCHAR(255), latest_version INT, create_time BIGINT, update_time BIGINT, visibility INT, status INT, extra VARCHAR(255));CREATE TABLE item_record ( item_id BIGINT, latest_version BIGINT, latest_status INT, latest_reason INT, latest_image_uris VARCHAR(255), latest_video_id BIGINT, latest_text_uri VARCHAR(255), update_time BIGINT, PRIMARY KEY (item_id, latest_version)); 主体数据存储我们选择minIO来存储： 配置项： 1234567minio: endpoint: 106.52.176.243 #Minio服务所在地址 port: 9000 #Minio服务的端口 bucketName: go-backend #存储桶名称 accessKey: 1GUH2mLGajNZTosgCiRa #访问的key，也可以是上面创建的AccessKey secretKey: foxeKloFgpuPdmuoIC5q18FqVEB3KkyPSMYPCqZs #访问的秘钥，也可以是上面创建的SecretKey upload: test #文件上传路径 minIO config： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.example.demo.config;import io.minio.MinioClient;import lombok.Data;import org.springframework.beans.factory.InitializingBean;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configuration@Datapublic class MinIOConfig &#123; /** * Minio 服务地址 */ @Value(&quot;$&#123;minio.endpoint&#125;&quot;) private String endpoint; /** * Minio 服务端口号 */ @Value(&quot;$&#123;minio.port&#125;&quot;) private Integer port; /** * Minio ACCESS_KEY */ @Value(&quot;$&#123;minio.accessKey&#125;&quot;) private String accessKey; /** * Minio SECRET_KEY */ @Value(&quot;$&#123;minio.secretKey&#125;&quot;) private String secretKey; /** * Minio 存储桶名称 */ @Value(&quot;$&#123;minio.bucketName&#125;&quot;) private String bucketName; /** * Minio 文件上传路径 */ @Value(&quot;$&#123;minio.upload&#125;&quot;) private String upload; @Bean public MinioClient minioClient()&#123; return MinioClient.builder() .endpoint(endpoint, port, false) .credentials(accessKey, secretKey) .build(); &#125;&#125; minIO工具类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185package com.example.demo.util;import com.alibaba.fastjson.JSONObject;import io.minio.*;import io.minio.http.Method;import io.minio.messages.DeleteError;import io.minio.messages.DeleteObject;import io.minio.messages.Item;import jakarta.servlet.ServletOutputStream;import jakarta.servlet.http.HttpServletResponse;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import org.springframework.util.FastByteArrayOutputStream;import org.springframework.web.multipart.MultipartFile;import java.util.ArrayList;import java.util.List;import java.util.stream.Collectors;@Componentpublic class MinIoUtil &#123; @Autowired private MinioClient minioClient; /** * 查看存储bucket是否存在 * @param bucketName 存储bucket * @return boolean */ public Boolean bucketExists(String bucketName) &#123; Boolean found; try &#123; found = minioClient.bucketExists(BucketExistsArgs.builder().bucket(bucketName).build()); &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; return found; &#125; /** * 创建存储bucket * @param bucketName 存储bucket名称 * @return Boolean */ public Boolean makeBucket(String bucketName) &#123; try &#123; minioClient.makeBucket(MakeBucketArgs.builder() .bucket(bucketName) .build()); &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; return true; &#125; /** * 删除存储bucket * @param bucketName 存储bucket名称 * @return Boolean */ public Boolean removeBucket(String bucketName) &#123; try &#123; minioClient.removeBucket(RemoveBucketArgs.builder() .bucket(bucketName) .build()); &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; return true; &#125; /** * 文件上传，并且返回url * @param file 文件 * @param bucketName 存储bucket * @return Boolean */ public String upload(String bucketName, MultipartFile file) &#123; try &#123; minioClient.putObject(PutObjectArgs.builder() .bucket(bucketName) .object(file.getOriginalFilename()) .stream(file.getInputStream(), file.getSize(), -1) .contentType(file.getContentType()) .build()); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; return getUrl(bucketName, file.getOriginalFilename()); &#125; /** * 文件下载 * @param bucketName 存储bucket名称 * @param fileName 文件名称 * @param res response * @return Boolean */ public void download(String bucketName, String fileName, HttpServletResponse res) &#123; GetObjectArgs objectArgs = GetObjectArgs.builder().bucket(bucketName) .object(fileName).build(); try (GetObjectResponse response = minioClient.getObject(objectArgs))&#123; byte[] buf = new byte[1024]; int len; try (FastByteArrayOutputStream os = new FastByteArrayOutputStream())&#123; while ((len=response.read(buf))!=-1)&#123; os.write(buf,0,len); &#125; os.flush(); byte[] bytes = os.toByteArray(); res.setCharacterEncoding(&quot;utf-8&quot;); //设置强制下载不打开 res.setContentType(&quot;application/force-download&quot;); res.addHeader(&quot;Content-Disposition&quot;, &quot;attachment;fileName=&quot; + fileName); try (ServletOutputStream stream = res.getOutputStream())&#123; stream.write(bytes); stream.flush(); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 查看文件对象 * @param bucketName 存储bucket名称 * @return 存储bucket内文件对象信息 */ public List&lt;JSONObject&gt; listObjects(String bucketName) &#123; Iterable&lt;Result&lt;Item&gt;&gt; results = minioClient.listObjects( ListObjectsArgs.builder().bucket(bucketName).build()); List&lt;JSONObject&gt; objectItems = new ArrayList&lt;&gt;(); try &#123; for (Result&lt;Item&gt; result : results) &#123; Item item = result.get(); JSONObject objectItem = new JSONObject(); objectItem.put(&quot;object_name&quot;,item.objectName()); objectItem.put(&quot;size&quot;,item.size()); objectItems.add(objectItem); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; return objectItems; &#125; /** * 批量删除文件对象 * @param bucketName 存储bucket名称 * @param objects 对象名称集合 */ public Iterable&lt;Result&lt;DeleteError&gt;&gt; removeObjects(String bucketName, List&lt;String&gt; objects) &#123; List&lt;DeleteObject&gt; dos = objects.stream().map(e -&gt; new DeleteObject(e)).collect(Collectors.toList()); Iterable&lt;Result&lt;DeleteError&gt;&gt; results = minioClient.removeObjects(RemoveObjectsArgs.builder().bucket(bucketName).objects(dos).build()); return results; &#125; /** * 获取文件url * @param bucketName 存储bucket名称 * @param filename 文件名称 * @return 文件url */ private String getUrl(String bucketName, String filename) &#123; try &#123; return minioClient.getPresignedObjectUrl(GetPresignedObjectUrlArgs.builder() .method(Method.GET) .bucket(bucketName) .object(filename) .build()); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125;&#125;","categories":[{"name":"项目学习","slug":"项目学习","permalink":"http://moonl0323.github.io/categories/%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/"},{"name":"后端开发","slug":"项目学习/后端开发","permalink":"http://moonl0323.github.io/categories/%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"后端开发","slug":"后端开发","permalink":"http://moonl0323.github.io/tags/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"},{"name":"架构设计","slug":"架构设计","permalink":"http://moonl0323.github.io/tags/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"}]},{"title":"Elysium-Link项目的系统架构详细设计","slug":"架构说明","date":"2024-11-30T16:00:00.000Z","updated":"2025-02-04T06:57:54.984Z","comments":true,"path":"2024/12/01/架构说明/","permalink":"http://moonl0323.github.io/2024/12/01/%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E/","excerpt":"","text":"Elysium-Link项目的系统架构详细设计用户关系服务业务架构说明 业务背景： 用户之间有互动交流，让用户可以更容易发现和关注其它用户的动态。 根据用户的关注可以做个性化推荐。 用户可以更容易发现自己和关注自己感兴趣的内容，留住用户。 服务接口职责 关注和取消关注接口：用户可以关注别的用户也可以取消对别的用户的关注。 查询用户关注列表接口：可以查看用户正在关注哪些其它的用户，按照最新的关注时间进行排序。 查询用户粉丝列表接口：可以查看用户有哪些正在关注他的人，按照最新的粉丝关注他的时间进行排序。 查询用户关注数和粉丝数接口：可以查看用户正在关注几个用户，有几个粉丝正在关注他。 查询用户之间关系的接口：比如可以查看用户1是否关注了用户2，用户2的粉丝是否包含了用户1等等。还有支持批量查询形式，比如查询用户1是否关注了指定的若干用户（批量查询关注关系），或者查询若干用户是否是用户1的粉丝（批量查询粉丝关系）。 基于Redis ZSET设计的具体操作与存在问题具体操作：数据结构选型：redis zset 维护对象： 一个是关注列表:Key设计为：following_{用户ID}，Member为被关注者的用户ID，对应的Score为关注行为发生的时间。 一个是粉丝列表:Key设计为：follower_{用户ID}，Member为粉丝用户ID，对应的Score为用户被关注行为发生的时间。 具体命令： 我们按照前面的五种接口来一一对应一下： 关注和取消关注接口：使用ZADD命令，如：用户1关注了用户2，具体命令如下： 12345//ZADD Key Score Member//操作关注列表ZADD following_用户1 时间戳 用户2//操作粉丝列表ZADD follower_用户2 时间戳 用户1 取消关注： 1234//ZREM Key Member//用户1取消关注用户2ZREM following_用户1 用户2ZREM follower_用户2 用户1 ​ 2.查询用户关注列表接口： 1234//ZREVRANGE Key start end 意思是查询该key列表的从start到end的所有member（从0开始），并且按照关注事件从近到远的排序//查询用户1的关注列表ZREVRANGE following_&#123;用户ID&#125; 0 -1//粉丝列表同理 ​ 3.同上 ​ 4.查询关注数和粉丝数 1//在redis中查询关注数就是查询关注列表的长度，也就是使用ZCARD命令 ​ 5.查询用户之间关系接口 ​ 5.1 单个查询：用户1是否关注了用户2，用户1是否为用户2的粉丝： 1234//用户1是否关注了用户2ZRANK following_用户1 用户2//用户1是否为用户2的粉丝ZRANK follower_用户2 用户1 ​ 5.2 批量查询：用户1是否关注了若干指定用户，若干指定用户是否为用户2粉丝： 12ZRANGE following_用户1 0 -1//先查出来用户1的关注列表如何看看指定用户在不在里面即可 存在问题：只用redis的话，由于redis是内存型数据库，那么在海量的用户量的时候，这个内存占用太多了，所有我们不考虑这种方案。 基于数据库设计单表设计​ User_relation 字段名 类型 含义 id BIGINT 自增主键，无特殊含义 from_user_id BIGINT 关注者用户ID to_user_id BIGINT 被关注者用户ID type INT 关注关系枚举 1表示正在关注，2表示取消关注 update_time DATE 记录修改时间 为了能够高效查询，我们需要建立索引。 索引建立考虑我们建立索引需要看具体的业务需要查询哪些数据，我们这里就是我们列的五种接口，除开第一个接口不是查询的，那么就是剩下四个查询接口，我们来一个一个看。 首先是查询用户关注列表，举个例子，要查用户1的关注列表，那么sql语句就是： 1SELECT to_user_id FROM User_relation WHERE from_user_id = 用户1 AND type = 1 ORDER BY update_time DESC; 显然我们需要为from_user_id建立索引。 同样的，我们列出其它几种接口的sql语句。 查询用户粉丝列表： 1SELECT from_user_id FROM User_relation WHERE to_user_id = 用户1 AND type = 1 ORDER BY update_time DESC; 查询用户之间关系： 用户1是否关注了用户2： 1SELECT 1 FROM User_relation WHERE from_user_id = 用户1 AND to_user_id = 用户2 AND type = 1; 用户2的粉丝是否包含了用户1： 1SELECT 1 FROM User_relation WHERE to_user_id = 用户2 AND from_user_id = 用户1 AND type = 1; 其它是同样的道理的，我们可以发现我们需要建立两种索引：1.from_user_id 2.to_user_id 那是不是我们只要建立这两种索引就够了，答案是否定的。 我们考虑大量用户量的情况。 假设拥有亿级的用户量，如果每个用户只是关注了1000个用户，那么我们的表都有千亿级别，显然不行，需要分库分表了。 我们考虑分表会发生什么情况。 我们考虑水平分表，那么我们一定是按照索引来分表的，为什么？ 我们先说说水平分表为什么要按照索引来分吧。 假设现在有一张表T(A,B,C); ​ T A B C a 1 a 3 b 2 b 4 c 6 现在我们设A是索引，我们考虑一下如果不按索引分表会发生什么，那么我们假设以B为标准划分表。 比如我们划分成三张表，1-2；3-4；&gt;4三种情况的表也就是： ​ T1 A B C a 1 b 2 ​ T2 A B C a 3 b 4 ​ T3 A B C c 6 那么考虑sql语句： 1SELECT * FROM T WHERE A=a 你看现在其实你需要去三张表里面都去找一遍。 但是如果你按照A索引分表，是不是就只要查一次就好了，因为大概率就是在某一张中。 那么回到我们实际的业务中呢，其实有两个索引，怎么办呢，答案是哪个作为标准分表都不合适。 比如你按照from_user_id索引进行分表： 1SELECT from_user_id FROM User_telation WHERE to_user_id = 用户1; 比如这条查询用户1的粉丝列表，那么就需要去n张子表都查一遍了。 反过来如果是查询用户1的关注列表，如果你按照to_user_id进行分表也是面临相同的问题。 那么该怎么办？ 我们试试竖直分表，但是字段本来就不多，那不如我们干脆拆开出来两张表，一张是following表，一张是follower表。 双表设计表结构如下： following表 字段名 类型 含义 id BIGINT 自增主键，无特殊含义 from_user_id BIGINT 关注者用户ID to_user_id BIGINT 被关注者用户ID type INT 关注关系枚举 1表示正在关注，2表示取消关注 update_time DATE 记录修改时间 follower表 字段名 类型 含义 id BIGINT 自增主键，无特殊含义 from_user_id BIGINT 关注者用户ID to_user_id BIGINT 被关注者用户ID type INT 关注关系枚举 1表示正在关注，2表示取消关注 update_time DATE 记录修改时间 你有没有发现他们其实和User_relation表是一模一样的，对的，就是一模一样的，我们刚刚分析问题是因为按照索引分表出现问题，那么我们这样拆开成两张表有个好处，我们两张表建立的索引不一样，他们负责的业务也不一样，这样子分表的时候就不会存在需要查大量子表的情况了。 比如，我们要查询用户的关注列表就可以使用following表，索引是from_user_id，这样分表后，由于只负责查询用户关注列表这一个业务动作，所以不会有任何问题，查询的数据大概率在同一张表之内，不会出现查询大量子表的情况。其它情况也是类似的。，比如查询用户的粉丝列表，那么查的就是follower表，索引是to_user_id，分表也不会出现问题。这几种接口都能命中索引，提高了查询的效率。 那么这种结构要求我们两张表的内容必须保持一致，也就是一张表必须是另一张表的copy，在创建和修改二者的数据时需要建立数据一致性关系。 这个好办，我们采用伪从技术，也就是follower表作为following表的伪从，消费following表产生的数据更新binlog。 架构图如下： binlog伪从技术一般来说选择阿里的Canal来做伪从技术也就够了。（Canal的原理也就是Mysql DRC的原理，我们暂不展开，感兴趣的以后会写篇专门介绍。） 但是了解到阿里团体已经基本“放弃”该项目了，所有我们考虑使用工业级别的方案使用Flink家族的FlinkCDC来做数据上下游同步。 最终选择FlinkCDC做上游数据，然后Kafka进行分发，下游各组件进行数据同步。 索引建立考虑我们先把前面那四种查询接口分类看看哪些需要Following表哪些是Follower表。 首先是查询用户关注列表, sql: 1SELECT to_user_id FROM T WHERE from_user_id = xxx AND type = 1 AND ORDER BY update_time; 显然建立from_user_id索引，那么也就是T是Following表。 我们看还需要type字段，同时还需要根据update_time进行排序，所以整个查询的过程应该是先查询from_user_id符合要求的，然后扫描type&#x3D;1的，最后对查询的用户还需要使用额外的空间进行排序，所以我们直接建立联合索引： 1KEY idx_following_list(from_user_id,type,update_time); 查询用户粉丝列表, sql: 1SELECT from_user_id FROM Follower WHERE to_user_id = xxx AND type = 1 AND ORDER BY update_time; 联合索引： 1KEY idx_follower_list(to_user_id,type,update_time); 查询用户1是否关注了用户2： sql： 1SELECT 1 FROM Following WHERE from_user_id = 用户1 AND to_user_id = 用户2 AND type = 1; 联合索引： 1KEY idx_following(from_user_id,to_user_id); 为什么不需要type字段做联合索引呢？因为我们是查到该记录就直接判断type是不是&#x3D;1就好了，而我们前面的查询关注列表，我们需要先根据from_user_id找到对应的记录，然后筛出type&#x3D;1的数据然后再进行排序，根本就是一个是SELECT 1只要判断是不是就行了，但是查询关注列表是SELECT to_user_id是一条条记录，所以需要type做联合索引。 查询用户2的粉丝是否包含用户1:sql： 1SELECT 1 FROM Follower WHERE to_user_id = 用户2 AND from_user_id = 用户1 AND type = 1; 联合索引： 1KEY idx_follower(to_user_id,from_user_id); 批量查询用户1是否关注了指定若干用户，比如是否关注了用户2，3，4： sql: 1SELECT to_user_id FROM Following WHERE from_user_id = 用户1 AND to_user_id IN (用户2，用户3，用户4); 联合索引： 1KEY idx_following(from_user_id,to_user_id); 同样的，批量查询用户2，3，4是否是用户1的粉丝： sql: 1SELECT to_user_id FROM Follower WHERE to_user_id =用户1 AND from_user_id IN (用户2，用户3，用户4); 联合索引： 1KEY idx_follower(to_user_id,from_user_id); 综上所述，每张表都各需要两个联合索引： Following表： 12KEY idx_following(from_user_id,to_user_id);KEY idx_following_list(from_user_id,type,update_time); Follower表： 12KEY idx_follower(to_user_id,from_user_id);KEY idx_follower_list(to_user_id,type,update_time); 虽然我们建立了多个联合索引占了空间，但是磁盘空间本身不值钱，所以这一波空间换时间其实是很划算的。 索引的回表问题及优化：我们前面不论是Following还是Follower表建立的索引都是非聚簇索引，也就是存储记录和索引分开了。 比如： 1SELECT 1 FROM Following WHERE from_user_id = 用户1 AND to_user_id = 用户2 AND type = 1; 这个走的索引是： 1KEY idx_following(from_user_id,to_user_id); 我们理清一下过程，首先走idx_following索引,查到对应的记录的主键了，接着走主键索引读它的type字段，这涉及了回表操作。 那么我们优化索引idx_following： 加多一个字段type： 1KEY idx_following(from_user_id,to_user_id,type); 同样的， idx_following_list查询用户的关注列表，首先查询联合索引，然后找到对应的主键id，回表查对应的to_user_id所以回表，改成; 1KEY idx_following_list(from_user_id,type,update_time,to_user_id); 同理，其它两种一样分析： idx_follower不需要改动： 因为我们要查的是： 1SELECT to_user_id FROM Follower WHERE to_user_id =用户1 AND from_user_id IN (用户2，用户3，用户4); 索引已经包含from_user_id了所以不用回表。 同样的改动idx_follower_list改成： 1KEY idx_follower_list(to_user_id,type,update_time,from_user_id); 缓存建立缓存数据选择 我们还是看前面的接口来说明： 首先是获取关注列表，我们知道一般在社交软件来说，我们会限制用户关注的人数，比如只能关注200人，现实业务也不会有人一直频繁关注别人的，所以我们可以做个关注人数上限，那么也就是说关注列表是固定最大大小的，我们可以才去全量缓存。模型和我们前面基于Redis ZSET的方案一致。 然后是获取粉丝列表，显然关注列表人数有上限，但是一个用户的粉丝数量是无上限的，一些大v甚至能达到亿级的量。所以全量缓存是不现实的。我们从实际业务出发，我们会发现大v是不会去查看全部粉丝的，往往就是查询前几页，也就是上万的数量级，所以我们可以缓存10000条最近的粉丝列表。如果要查&gt;10000条外的就去查数据库就好了，但是这里会有安全问题，如果有人恶意攻击发送大量查询10000条外的粉丝的请求，那么数据库会崩，所以我们做限流，先查看当前请求是不是查询10000条之外的数据，如果是，就检查当前请求量是否已经达到限流阈值了，如果超过了直接拒绝执行，因为大概率是恶意攻击。 查询用户之间的关注关系，这个简单，比如查询用户1是否关注了用户2，用户3等，只要去查询关注列表里面有没有这些用户就好了。 麻烦的是如何批量查询一些用户是否是用户1的粉丝，如果用户1粉丝很少那还好，毕竟缓存命中率很高，万一很多怎么办，那么很大几率就是我们要查的一些用户它不在缓存里面。有一种解决方法就是反查关注列表，什么意思呢？举个例子，现在我们用户1有很多粉丝，存了最近的10000个在redis中的粉丝列表，现在我要查询100个用户是否是用户1的粉丝，我们发现只命中了10个，也就是剩下90个我不确定是不是用户1的粉丝，因为万一是10000个开外的呢。所以这个时候进行反查，我开90个线程让去查90个用户各自的关注列表里面有没有用户1，如果有那就是他的粉丝，没有就不是。问题很显然，万一很多，线程会爆掉，而且读请求被放大了。那还有什么方法呢？我们开多一个缓存去缓存粉丝关系，数据结构不是zset而是hash，其实Key是用户1，Field就是我要查的若干用户比如用户2，用户3，用户4等，Value就是1表示是粉丝，0表示不是粉丝。如果发现这个缓存里面有至少一个用户不在这个Hash中，那就得查数据库了。比如，我缓存了hash用户1，field是用户2，用户3。现在我查的其中有个用户4，我发现不在这里面，那么之前去查数据库。这个方案的问题就是redis空间会占的多，请求越多，这个缓存列表越大，所以要设置好过期时间。 缓存建立和更新策略创建缓存比较简单，就是我发出请求发现缓存有就查缓存，没有就查数据库，然后建立缓存。 那么更新策略呢？ 我们采取以往的，如果更新了，那么先更新数据库，然后删缓存，听起来没什么太大毛病。我们考虑极端情况。 因为大V来说经常有人会关注他的，如果和查看粉丝列表高并发执行的话。 比如交替进行： 有用户读取粉丝列表，发现没有缓存，于是去查数据库，然后建立缓存。 有新用户关注该大V，更新数据库，然后删除缓存。 又有用户读取粉丝列表，又重新建缓存。 。。。 我们会发现，很可能就是前脚刚建立缓存，下一秒就删掉了，这样子频繁的删除创建肯定不行的。所以粉丝列表绝对不能这么做。 那我们该怎么办呢？ 我们考虑一下，我们要的是缓存和数据库数据一致对吧，那么我们还是直接利用伪从技术，我们创建专门的消费者服务作为数据库Following表的伪从，这样子数据库Following表有变更，比如发生关注，取消关注事件了，消费者服务就会收到最新的数据变更binlog，然后去修改缓存。如果缓存存在就修改缓存。 我们现在考虑查询关注列表，由于在实际业务当中，没有人会一下子去关注很多人，所以关注列表比较稳定，所以之前先更新数据库删缓存的策略可以用，当然你也可以用伪从技术。 本地缓存我们在实际业务当中，往往会额外关注大V的关注列表，比如某某明星关注了谁，所以大V等公众人物的关注是很谨慎的，意味着他们的关注列表不会经常变化，但是读请求往往会很大，因为很多人好奇嘛，所以我们可以进一步建立关注列表的本地缓存，从而减少访问Redis，进一步提高访问性能。 普通用户的话关注了谁其实没有多少人关心，所以读请求不大，不需要本地缓存。 粉丝列表的话，大V的粉丝变动频繁，所以不适合本地缓存，普通用户的话，访问少，所以也不需要。 计数服务 缓存+数据库方案最终架构关注&#x2F;取消关注接口：直接更新数据库的Following表即可响应用户，过程异步。Follower表，计数服务，Redis缓存都会依赖Following产生的binlog表分别更新数据。 比如，用户1关注了用户2： 1.首先直接更新Following表的from_user_id,to_user_id,type和update_time字段。 2.Follower表会使用from_user_id(用户1)，to_user_id(用户2),关注时间。 3.计数服务会计数消费者请求计数服务增加用户1的关注数和用户2的粉丝数。 4.Redis缓存会缓存消费在用户1的关注列表缓存中加入用户2，在用户2的粉丝列表缓存中加入用户1. 查询用户关注列表接口： 先查本地缓存，没有就查Redis，还没有就查数据库，然后创建缓存。然后我们检查该用户的粉丝数是否达到一定阈值，如果是就是大V，我们将其关注列表加入本地缓存。 查询用户粉丝列表： 如果查的是最近10000个以内，那就直接查Redis，没有就查数据库，然后创建缓存。如果要查10000个开外的，就查看是不是达到请求阈值了，超过了就拒绝访问了，没超过就去查数据库。 查询用户关注数和粉丝数：直接去计数服务里面获取。 查询用户关系接口： 简单查询用户1是否关注了用户2：直接查看用户1的redis缓存的关注列表里面有没有用户2。如果换成不存在就查数据库，然后建立换成。 如果是批量查询用户1是否关注了若干用户：流程同上，只不过是获取到用户1的关注列表数据后，从中选出指定的用户。 查询若干用户是否是用户1的粉丝也就是查询若干用户是否关注了用户1： 如果用户1的粉丝数&lt;&#x3D;10000说明，粉丝列表是全量缓存的，直接查看粉丝列表缓存里面有没有这些用户，不用查数据库，因为没有在缓存里面说明就是不是粉丝。 如果用户1的粉丝超过了10000个，说明你查粉丝列表缓存没有，不代表就不是粉丝，因为缓存只存了10000个。所以只能用粉丝关系缓存hash了，我们查看hash缓存，看看这些用户在不在这里面，如果在直接看他们的值是不是1，是就代表这个用户是粉丝，0就表示该用户不是粉丝，直接返回结果。不需要查数据库。如果有用户不在hash中，我们就得回源数据库Follower表查询该用户和用户1的关系，将结果写到hash对象中去。 架构图如下： 拓展：加入图数据库NoSQL 内容发布系统设计背景及实际问题1.内容是形式多样性：可以是短文字，长文字，图片，音频，视频等，怎么存？ 2.发布内容可能存在问题，如何检测审核？ 3.内容渠道很多，比如主页，推荐流，好友动态，搜索框等，怎么样让这些内容渠道知道用户发布的内容呢？ 4.内容一般读多写少，怎么样做到高并发？ 内容存储设计我们一般数据库用关系型数据库mysql，但是这会有几个问题： 1.首先，只能存字符串类型的数据，并不支持存图片，音视频呀这些，除非暴力转。 2.哪怕是字符串类型，只要是长文本就不太适合。因为我们拿InnoDB引擎举例子，内部是使用B+树来存的，数据一般存到叶子节点，如果太长了就会占很多空间，影响查询效率。所以实际上InnoDB会将真正的数据放在磁盘的另一个地方，也就是说叶子节点存的是引用。所以这样子就会多一次IO了，影响效率。 所以综上不应该用mysql存所有数据，但是我们可以模仿InnoDB，也就是表是mysql的，存的是元数据，实际的一些比如长文本呀，音视频是存在别的地方的。 内容元信息设计：我们先来设计元数据表： 我们要两张表，一张是信息表item_info：存的是真正决定对外发布的内容元信息。一张是内容修改历史记录表item_record：用于应对内容的修改操作，每次修改后的内容数据都会先存到这里面来，以便提供内容修改历史记录的查询途径，最重要的是，等待此内容修改被审核通过后再写到信息表中。 item_info表： 字段名 类型 含义 item_id int64 内容唯一标识，主键 creator_id int64 特指创作者ID，是特殊的用户ID online_version int 线上内容版本号 online_image_uris string 线上内容的相关图片URI列表，其被序列化为String形式 online_video_id int64 线上内容的相关视频的唯一标识 online_text_uri string 线上内容的相关长文本URI，其被序列化为String形式 latest_version int 最新变更的内容版本 create_time int64 内容创建时间 update_time int64 内容变更时间 visibility int 线上内容的可见范围：私密，好友可见，粉丝可见，所有人可见 status int 内容的状态：待审核，正常展示，被删除，被下架 extra string 其它可拓展字段的键值对形式的序列化，应对个性化需求 item_record表： 字段名 类型 含义 item_id int64 此次变更操作关联的内容ID latest_vesion int64 此次变更内容的版本号 latest_status int 枚举此次变更内容的审核状态：待审核，审核通过，审核拒绝 latest_reason int 如果此次变更内容的审核状态是拒绝。则此字段枚举了审核拒绝的原因，比如涉恐涉黄侵权等 latest_image_uris string 与此次变更内容相关的图片URI列表，其被序列化为String形式 latest_video_id int64 与此次变更内容相关的视频唯一标识 latest_text_uri string 此次变更内容的最新长文本文件 update_time int64 此次内容的变更时间 之所以需要两张表是因为每次内容变更内容后，都不一定能够立刻将内容发布上线，所有item_info表主要存的是内容基本信息。而item_record主要存的是内容变更记录，审核后才会将变更记录替换到item_info中去，相当于暂存待生效的内容元信息。 内容主体存储设计：短文本我们可以用分布式kv存储系统或者分布式文档数据库，后者比较简单使用。这里我们使用redis进行存储。 长文本&#x2F;图片&#x2F;音视频 我们使用分布式文件存储或者分布式对象存储系统来存。这里我们使用minio进行存储 音视频转码：音视频转码可以提供不同质量的音视频以满足不同用户的硬件设备和网络需求，先不做。 内容审核设计（审核中心系统）：什么时候送审？如果只要用户发布内容或者修改内容就送审的话，显然虽然这种方案是最保险的但是高并发的情况下，显然是审核不完的，很消耗资源和时间。 所以我们换种思路： 我们来思考一下： 1.什么样体量的内容更容易造成负面影响？显然是要么内容曝光度很高要么是这个作者很多粉丝。 2.说明样的人发布的作品更可能违规？显然是那些经常被举报或者他之前的作品经常被拒绝通过的。 所以我们按照这两个思路设计: 1.首先粉丝超过10000人的作者发布或者修改内容时要送审。 2.如果最近远端时间被举报超过10次或者最近发布的作品被拒绝超过10次的用户发布新内容或者修改时要送审。 3.如果某个内容浏览量超过500次，点赞点踩或者收藏量评价量超过300次的内容，当这个内容修改时要送审。 4.如果有3个用户以上投诉的内容，也要送审。 如何审核内容？人工智能和人工审核结合，先不展开。 审核中心对外交互：发布系统将内容发送到审核中心，审核中心将内容的审核结果返回给内容发布系统，所以二者适合使用基于消息队列的异步化形式进行交互。 需要两个消息队列主题： 消息队列主题 含义 生产者 消费者 event_audit_content 内容送审 内容发布系统 审核中心 event_audit_result 内容审核完成 审核中心 内容发布系统 交互具体流程如下： 1.当内容满足审核条件时（比如流量大的作品，大v发布，内容被投诉），内容发布系统会向event_audit_content消息队列发送消息，消息内容是item_id（内容唯一标识）和version（内容版本号），用于告知审核中心需要审核某内容的某版本。 2.审核中心收到event_audit_content新内容之后，根据item_id获取内容的全部信息然后排队审核。 3.内容审核完成之后，发送到event_audit_result消息队列中去，消息内容是：item_id（内容唯一标识），version（内容版本号），audit_result（审核结果）和reason（不通过的原因，如果审核结果是“拒绝通过的话”）。 4.内容发布系统收到event_audit_result中新消息后，会做相应的处理。 内容全生命周期管理设计内容创建设计：1.如果新创建的内容包含图片，就先将图片上传到后端的分布式对象存储系统中，然后返回一个URI列表image_uris。如果包含长文本，也是上传返回对应URI text_uri。 2.如果包含视频，就上传到专门的视频服务，返回video_id（先不做）。 3.客户端携带创作者ID,内容文本，image_uris，text_uri，video_id信息请求内容发布系统。 4.为新创建的内容生成一个基于时间戳的唯一ID，作为内容的唯一标识。 5.将内容元信息存到item_info表中，填充item_id，creator_id，visibility，create_time,update_time字段，如果因为创作者条件需要审核就设置online_version字段为0，表示暂未创建成功，并将latest_version设置为1，作为此次内容的版本。将status设置为“待审核”。如果不需要审核就是直接将online_version和latest_version设置为1，表示内容发布了。 6.如果内容包含短文本，就将短文本存到redis中去，key为{item_id}_{version}，value就是短文本内容。 7.将信息存到item_record中，如果需要送审就将latest_status设置为“待审核”，如果不需要直接设置成“审核通过”。 8，如果需要审核就按之前说的异步推到消息队列。 架构图如下： 内容修改设计：流程和前面的创建差不多，只不过是修改的适合要考虑是否要送审。 如果修改之前已经达到送审的播放量，互动量的条件阈值，那么这一次修改需要送审： 1.有新增的图片，长文本，替换了新视频等等都是经过分布式对象存储系统。 2.填充字段。 3.查item_info中latest_version字段+1。 4.将元信息填到item_record表，设置item_record中的latest_status设置为”待审核“。 5.然后发给审核中心。 如果不需要审核： 前面基本一样，然后将元信息存到item_record表中，设置其中latest_status字段为”已上线“。 如果创作者没有改动内容本身，比只是调整了内容的可见性，则直接更新item_info表即可。 内容审核结果处理与版本控制设计：内容发布系统在收到event_audit_result消息队列的审核结果之后： 1.如果审核结果是通过，那么就将item_record表中的item_id，version的暂存数据覆盖到item_info表中去，表示内容发布了。 2.覆盖审核结果是不通过，那么就要设置item_record中latest_status为”审核拒绝“，并且填原因到latest_reason。 问题：假设我有用户短时间做了三次更新，编号分别是1，2，3，假设审核时间是1&gt;2&gt;3，那么就是最终的结果是1覆盖了所有，但是其实我们希望的是最后一次更新去覆盖，也就是说，我们要保证消息队列消息的顺序。 解决：很简单，因为版本号是递增的，所以我们去对比版本号就好了。 具体如下： 如果审核拒绝了，那么不进行下一步，如果通过了，如果online_version&#x3D;version,说明此次审核内容是线上内容可能达到阈值了，将结果写入item_info中的status字段。 如果online_version&lt;version说明审核结果版本比现实的新，就item_record替换item_info表数据就好了。 如果online_version&gt;version说明审核结果版本落后了，就继续下一步了。 流程图如下： 内容删除和下架设计：采用软删除，直接标记item_info表中的status为”被删除“或者”被下架“。 内容分发设计我们内容发布系统应该和内容分发渠道进行解耦，所以自然而然的会想到使用消息队列来解决。 如图： 内容展示设计我们这里直接给出架构图： 完整的整个服务的架构图： 通用计数系统业务场景：1.用户本身：关注数，粉丝数，作品数，热度等； 2.用户发布的作品：点赞数，分享数，评论数，转发数，收藏数等； 3.评论：每条评论有自己的评论数，点赞数，点踩数等； 计数数据的特点：1.读请求量巨大； 2.写请求量巨大：因为用户触发例如点赞的动作成本很低，一个作品往往可能同一时间很多人点赞，所以写请求量巨大； 3.非产品的绝对强依赖：在极端情况下，由于这些计数非核心功能，如果短时间不能用并不大会影响用户的心情； 4.对数据的精确性要求与数值的增加成反比：比如说点赞数，在一个作品的点赞数少的情况下用户可能会比较关心该作品的实际点赞数，但是当作品的点赞数很大的情况下，大家只会关注数量级，比如1000w的点赞量，哪怕实际上是1000w过1000个，其实你展示成1000w个也不会有影响； 计数数据的存储选择：关系型数据库（MySQL为例）存在的问题：如果我们使用mysql作为计数数据的存储，我们拿点赞作为业务场景举例说明。假设我们现在要给某个作品进行点赞，那么我们会记录某作品被哪个用户点赞过的这一条数据。如果我们要查询该作品的点赞总数就需要执行SQL中count(1)语句。我们知道在mysql中的不同引擎下的实现方法不一样。 如果是MyISAM引擎的话，会将一个表的总行数存储到磁盘上，因此执行count(1)的时候会直接返回这个数，效率很高。但是由于InnoDB引擎的事务保证，所以实际开发中我们一般采用的是InnoDB，这个引擎执行count(1)的时候需要把数据一行行的从引擎中读出来，然后进行累计计数。。在业务量巨大的情况下，这个方法的性能极差。原因就在于其实我们只是要一个总数而已，我们并不关心每一行有什么数据。那么我们是否可以学习MyISAM引擎将总数单独存在一张表当中呢？其实还是不行，因为写请求量很大，高并发的写请求容易把数据库冲垮。 我们引出另一个问题： 是否要使用关系型数据库？一般来说，如果关系型数据库无法应对写压力的话，其实可以使用异步的方式对数据进行更新。先更新缓存，然后通过消息队列异步通知数据库进行数据更新。如果关系型数据无法应对高并发的读压力的话，就可以让缓存先顶上。（因为计数不像金融这些要求强一致性） 表面上看，这一套方案似乎没有任何的问题，但是仔细一想，在实际的业务中，几乎任何时间都是在和缓存直接交互的，所以似乎在缓存之后再去维护一个关系型数据库有点多此一举。引入关系型数据库无法是看重了在数据量大的情况下，存在mysql中成本比较低，因为是磁盘。另外看重了mysql的持久化。但是计数来说，首先数据都是一个数字，数据量并不是很大，哪怕是很多用户的情况下，每个用户就记录几个数字。另外，计数数据并不是绝对不能够丢失的数据，因为可以从数据记录流水总数去反推。所以在这里我们干脆使用redis作为数据存储系统。又可以满足高并发，又减少了没必要的维护。整体的架构也会变的更加简洁和易维护。 使用Redis存储计数数据我们来说说前面提到的几个问题，首先是高并发，显然Redis完全可以胜任，因为首先Redis是一个基于内存的单进程Reactor高性能服务器，并且在业界有很多成熟的Redis分布式集群架构（如Codis），可以提供较好的高可用性和可扩展性。 第二个问题是持久化问题，我们知道Redis提供了AOF和RDB两种方式来保证数据的持久性。即使在极端的情况下，可能出现1s数据的丢失，也可以接受这部分数据的丢失，并且我们可以用异步检查，周期性的使用与计数相关的数据记录总行数来修正丢失的计数。 Redis数据类型的选择为什么String不好？我们举个例子，现在我们的业务场景是对一个作品进行计数服务，那么这个作品就有评论数，点赞数，分享数，转发数，收藏数等，我们可以使用5个String来保存这一个作品的计数。比如key值的前缀都可以是count_{content_id}。如下表： 数据 Redis String Key 评论数 count_{content_id}__comment 点赞数 count_{content_id}__like 分享数 count_{content_id}__share 转发数 count_{content_id}__forward 收藏数 count_{content_id}__collect 如果现在要点赞了，执行的命令就是： 1INCRBY count_&#123;content_id&#125;__like 1 这样的设计十分简单，但是缺点也很明显。 1.首先如果用户量十分巨大的情况下，redis系统是以集群的方式对外服务的，那么这意味着，当执行MGET命令获取N个Key，会启动最多N个线程去对应的Redis节点上获取数据。如果作品有大量的读取请求的话，Redis集群会有较大的线程资源开销，而且如果有一个线程失败了，整个MGET命令都会失败，计数服务的可用性就会急剧下降。 2.一个作品就要五个String对象，那么一亿个用户应该会对应差不多上百亿的作品，本来内存就宝贵，现在为了记录一些数字内存消耗太高了。 那么这两个问题怎么解决呢？ 解决：针对第一个问题其实业界中比如Codis就提供了很好的解决方案，他可以为相同前缀的key打上hashtag表桥，集群会保证命中相同hashtag的redis key会被存在集群中的同一个redis服务节点上。所以我们可以保证同一个作品的各个String对象都会分配到同一个redis节点上面去，这个时候我们使用mGET命令的话也仅仅会被一个Redis节点执行，和单机的情况是一样的。 针对第二个内存问题的话，我建议使用Hash对象来存。 使用Hash对象来存储即使我们解决了第一个问题，但是还是建议直接用Hash来存。原因如下： 1.Hash对象可以很方便的将某个场景下的全部计数数据都以Hash Field的形式汇聚到同一个Hash Key下。此时Redis Hash Key就是count_{content_id}，Field就是comment，like，share，forward，collect，value就是对应的数量。 2.更加节省内存，因为根据redis低层的数据结构来说，当Hash对象存储少于512个kv对，且总大小不超过64字节的时候，采用的是压缩列表。首先我们的field不可能超过512个，其次都是数字，也就是int64类型，所以低层一定是压缩列表实现的。 内存的进一步优化Hash对象可以进一步节省内存吗，答案是可以的。我们举个列子，比如我们之前提到的作品有五个数据。其实不管是哪个作品都是这五个量，但是我们却每次都要存对应的Field值，也就是说，只要我们事先约定好顺序，那么Field的值是不需要存的。 如果是存储一个作品的相关计数，我们采用长度为5的数据，因为哪怕是一个国民级别的应用也很难做到计数值超过100亿，所有我们5个字节也就是2的40次方（这个数远超100亿了）。所有我们最终采用的就是长度为5的int5类型的数组。 那么我们还需要什么吗？ 技术服务组件架构首先我们需要一个元数据中心，允许不同的开发者注册自己使用场景的信息，也就是告诉系统我要存多少数据，每个数据代表什么含义。 而在计数服务的Redis存储集群中，每个Redis节点本地都缓存了元数据中心的全部信息，然后执行命令之前进行语义分析。 比如说我们对content111点赞，那么首先会将对应的redis命令发送到计数服务器，然后收到这条命令之后会去对应数据中心，然后对对应的字段进行操作。 我们采用自己定制化的redis结构大概节约了68%的内存空间（但是很难实现需要修改源码。）。 冷热数据分离在实际业务中，有些数据可能会很少人访问，相反有些数据就可能会大量访问，但是他们的存储却是对等的。考虑到内存的宝贵，我们将冷数据存到磁盘，这里我们可以考虑使用RocksDB存储引擎，就可以将kv数据存到磁盘。我们可以选用开源项目Pika来解决redis因为存储量巨大导致内存不够的问题。 应对过热数据有冷数据就会有热数据，比如某个大V突然登录某个平台（b站经常有），那么可能很多人会同一时间去关注这个人，假设每次计数操作都是一条命令，那么就可能会同时有多条命令要执行。那么这个节点可能会挂。我们这里可以采取异步写和写聚合的方式解决。写聚合就是将多条命令变成一条，可以结合Lua脚本解决。 计数服务的整体结构 组件解释如下： 计数服务：对外提供服务，直接和Redis集群交互，也是消息队列的生产者，实现计数更新的异步写。 消息队列：负责异步写请求的接受和分发。 计数更新：消息队列的消费者，接受异步写请求，然后做好聚合写之后请求计数存储集群。 Redis Proxy：提供计数存储集群，是Redis集群与分布式磁盘KV数据库对外提供服务的总代理。 在线Redis集群：存储全部计数数据并向外提供计数读&#x2F;写能力。如果做了数据冷热分离，就存热门的数据。 冷数据分布式KV存储系统：存冷数据，对外提供计数读写能力。 计数系统的适用范围如果计数要求不精准可以使用，如果是金融属性这种需要强一致性的就不太适合用计数服务了。另外如果用户数据的访问权限只有用户自己，并且请求量也不大，直接使用关系型数据库就好了。 Timeline Feed服务Feed流介绍Feed流就是一种以时间线为基础的信息流展示形式，把用户感兴趣的内容呈现在用户的Feed页面上。 Feed流分类主要有三种： 1.推荐Feed流：按照你平时的浏览兴趣聚合内容，你可以不认识发布者，但是他发布的你很可能感兴趣； 2.关注Feed流：你关注的用户发布的内容被聚合为Feed流并且按照内容的发布时间从近到远的展示你所关注的那些人最近发布的内容。例如微信的朋友圈，微博的主页等； 3.附近Feed流：就是把你附近用户最新发布的内容； 推荐Feed流主要依赖于推荐算法，其实应该和深度学习框架结合。附近Feed流重点是地理位置的判断，技术差异太大了。所以我们只讨论最具有通用性的基于时间的关注Feed流。 Timeline Feed流的主要功能Timeline Feed流提供的数据应该是我们所关注的人在指定时间段内发布的内容列表，并且内容按照时间由近到远排列。 用户在客户短浏览Timeline Feed页面时一般由两种操作方式： 1.下拉：也就是刷新Feed流，拉取当前时间最新的N条Feed流； 2.上滑：拉取更早时间的N条Feed流； 需要注意的是，用户首次进入Timeline Feed页面的时候，显示的应该是当前时间最新的Feed流，也就是和下拉效果一样的。 虽然我们可以一直上滑来获取历史记录，但是我们应该做一个限制，限制用户不能无止境的获取历史数据。 拉模式与用户发件箱 拉模式的意思就是每当用户获取Feed流的时候都会主动去关注者的发件箱里面拉取内容。而内容发布者也就是用户关注的人每次发布内容的时候都只会将内容写到自己的收件箱里面去，等待用户主动拉取。 显然这样会有很明显的问题，首先一次用户请求需要N次的读请求，N就是你关注的列表数，这个被称之为“读扩散”，也就是一次请求扩大变成了N次读请求。高并发场景下可能会击垮存储用户内容的数据库。 推模式与用户收件箱 这个模式下，当用户发布了新的内容后会给粉丝列表中的粉丝的收件箱都发送一份内容。这样子粉丝去获取Feed流的时候直接去自己的收件箱里面获取就好了。这个方案显然也会存在问题： 1.存储压力大，一份内容得存N份，内存存储扩大了N倍。 2.写扩散，发布一条内容得发起N次写请求，显然可能会击垮收件箱所依赖的数据库。 推拉结合模式拉模式没有存储压力但是读扩散，推模式有存储压力和写扩散，但是没有读压力。所以结合一下。 首先我们要明确一点，就是我们如果有充足的存储资源，也就是我们更加应该考虑用户体验的情况下，应该尽量使用推模式这个原则，因为拉模式的读扩散的性能我们是没办法接受的。 我们通过用户的粉丝数来决定用哪种模式，如果粉丝不多，那么即使我们用推模式，那么写请求和存储的个数都是可控的，这个时候我们可以直接使用推模式。如果是大V的话，粉丝数巨大，大V在发布内容的时候直接采用拉模式只把自己的内容写到自己的发件箱里面去，当用户需要获取Feed流的时候会去主动拉取的。 我们还可以进一步做区分，哪怕是大V我们也可以采用推模式，因为不是所有粉丝都是活跃用户，我们可以主动推给活跃用户，而不活跃用户则不主动推而是他们要获取的时候自己去拉。 如何具体实现Timeline Feed服务我们自然而然的会想到在内容发布完结束后触发Timeline Feed服务，但是这样有个问题，这样子的话我们将Timeline Feed服务耦合到了内容发布服务系统里面去了。并且可用性也很差，如果内容发布服务系统需要更新或者升级什么的，会中断正在推送的服务。比如说现在服务实例正在向1000各用户的收件箱中推送内容，但是执行完100个的时候刚好内容发布系统要更新于是实例退出了，此时会有900个用户没有收到此内容，显然不可靠。 所以我们要将二者充分解耦，我们借助消息队列进行解耦。","categories":[{"name":"项目学习","slug":"项目学习","permalink":"http://moonl0323.github.io/categories/%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/"},{"name":"后端开发","slug":"项目学习/后端开发","permalink":"http://moonl0323.github.io/categories/%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"后端开发","slug":"后端开发","permalink":"http://moonl0323.github.io/tags/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"},{"name":"架构设计","slug":"架构设计","permalink":"http://moonl0323.github.io/tags/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"}]},{"title":"内容发布系统","slug":"内容发布系统","date":"2024-11-30T16:00:00.000Z","updated":"2025-02-04T06:58:43.445Z","comments":true,"path":"2024/12/01/内容发布系统/","permalink":"http://moonl0323.github.io/2024/12/01/%E5%86%85%E5%AE%B9%E5%8F%91%E5%B8%83%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"内容发布系统 设计背景及实际问题1.内容是形式多样性：可以是短文字，长文字，图片，音频，视频等，怎么存？ 2.发布内容可能存在问题，如何检测审核？ 3.内容渠道很多，比如主页，推荐流，好友动态，搜索框等，怎么样让这些内容渠道知道用户发布的内容呢？ 4.内容一般读多写少，怎么样做到高并发？ 内容存储设计我们一般数据库用关系型数据库mysql，但是这会有几个问题： 1.首先，只能存字符串类型的数据，并不支持存图片，音视频呀这些，除非暴力转。 2.哪怕是字符串类型，只要是长文本就不太适合。因为我们拿InnoDB引擎举例子，内部是使用B+树来存的，数据一般存到叶子节点，如果太长了就会占很多空间，影响查询效率。所以实际上InnoDB会将真正的数据放在磁盘的另一个地方，也就是说叶子节点存的是引用。所以这样子就会多一次IO了，影响效率。 所以综上不应该用mysql存所有数据，但是我们可以模仿InnoDB，也就是表是mysql的，存的是元数据，实际的一些比如长文本呀，音视频是存在别的地方的。 内容元信息设计：我们先来设计元数据表： 我们要两张表，一张是信息表item_info：存的是真正决定对外发布的内容元信息。一张是内容修改历史记录表item_record：用于应对内容的修改操作，每次修改后的内容数据都会先存到这里面来，以便提供内容修改历史记录的查询途径，最重要的是，等待此内容修改被审核通过后再写到信息表中。 item_info表： 字段名 类型 含义 item_id int64 内容唯一标识，主键 creator_id int64 特指创作者ID，是特殊的用户ID online_version int 线上内容版本号 online_image_uris string 线上内容的相关图片URI列表，其被序列化为String形式 online_video_id int64 线上内容的相关视频的唯一标识 online_text_uri string 线上内容的相关长文本URI，其被序列化为String形式 latest_version int 最新变更的内容版本 create_time int64 内容创建时间 update_time int64 内容变更时间 visibility int 线上内容的可见范围：私密，好友可见，粉丝可见，所有人可见 status int 内容的状态：待审核，正常展示，被删除，被下架 extra string 其它可拓展字段的键值对形式的序列化，应对个性化需求 item_record表： 字段名 类型 含义 item_id int64 此次变更操作关联的内容ID latest_vesion int64 此次变更内容的版本号 latest_status int 枚举此次变更内容的审核状态：待审核，审核通过，审核拒绝 latest_reason int 如果此次变更内容的审核状态是拒绝。则此字段枚举了审核拒绝的原因，比如涉恐涉黄侵权等 latest_image_uris string 与此次变更内容相关的图片URI列表，其被序列化为String形式 latest_video_id int64 与此次变更内容相关的视频唯一标识 latest_text_uri string 此次变更内容的最新长文本文件 update_time int64 此次内容的变更时间 之所以需要两张表是因为每次内容变更内容后，都不一定能够立刻将内容发布上线，所有item_info表主要存的是内容基本信息。而item_record主要存的是内容变更记录，审核后才会将变更记录替换到item_info中去，相当于暂存待生效的内容元信息。 内容主体存储设计：短文本我们可以用分布式kv存储系统或者分布式文档数据库，后者比较简单使用。 长文本&#x2F;图片&#x2F;音视频 我们使用分布式文件存储或者分布式对象存储系统来存。 音视频转码：音视频转码可以提供不同质量的音视频以满足不同用户的硬件设备和网络需求，先不做。 内容审核设计（审核中心系统）：什么时候送审？如果只要用户发布内容或者修改内容就送审的话，显然虽然这种方案是最保险的但是高并发的情况下，显然是审核不完的，很消耗资源和时间。 所以我们换种思路： 我们来思考一下： 1.什么样体量的内容更容易造成负面影响？显然是要么内容曝光度很高要么是这个作者很多粉丝。 2.说明样的人发布的作品更可能违规？显然是那些经常被举报或者他之前的作品经常被拒绝通过的。 所以我们按照这两个思路设计: 1.首先粉丝超过10000人的作者发布或者修改内容时要送审。 2.如果最近远端时间被举报超过10次或者最近发布的作品被拒绝超过10次的用户发布新内容或者修改时要送审。 3.如果某个内容浏览量超过500ci，点赞点踩或者收藏量评价量超过300次的内容，当这个内容修改时要送审。 4.如果有3个用户以上投诉的内容，也要送审。 如何审核内容？人工智能和人工审核结合，先不展开。 审核中心对外交互：发布系统将内容发送到审核中心，审核中心将内容的审核结果返回给内容发布系统，所以二者适合使用基于消息队列的异步化形式进行交互。 需要两个消息队列主题： 消息队列主题 含义 生产者 消费者 event_audit_content 内容送审 内容发布系统 审核中心 event_audit_result 内容审核完成 审核中心 内容发布系统 交互具体流程如下： 1.当内容满足审核条件时（比如流量大的作品，大v发布，内容被投诉），内容发布系统会向event_audit_content消息队列发送消息，消息内容是item_id（内容唯一标识）和version（内容版本号），用于告知审核中心需要审核某内容的某版本。 2.审核中心收到event_audit_content新内容之后，根据item_id获取内容的全部信息然后排队审核。 3.内容审核完成之后，发送到event_audit_result消息队列中去，消息内容是：item_id（内容唯一标识），version（内容版本号），audit_result（审核结果）和reason（不通过的原因，如果审核结果是“拒绝通过的话”）。 4.内容发布系统收到event_audit_result中新消息后，会做相应的处理。 内容全生命周期管理设计内容创建设计：1.如果新创建的内容包含图片，就先将图片上传到后端的分布式对象存储系统中，然后返回一个URI列表image_uris。如果包含长文本，也是上传返回对应URI text_uri。 2.如果包含视频，就上传到专门的视频服务，返回video_id（先不做）。 3.客户端携带创作者ID,内容文本，image_uris，text_uri，video_id信息请求内容发布系统。 4.为新创建的内容生成一个基于时间戳的唯一ID，作为内容的唯一标识。 5.将内容元信息存到item_info表中，填充item_id，creator_id，visibility，create_time,update_time字段，如果因为创作者条件需要审核就设置online_version字段为0，表示暂未创建成功，并将latest_version设置为1，作为此次内容的版本。将status设置为“待审核”。如果不需要审核就是直接将online_version和latest_version设置为1，表示内容发布了。 6.如果内容包含短文本，就将短文本存到redis中去，key为{item_id}_{version}，value就是短文本内容。 7.将信息存到item_record中，如果需要送审就将latest_status设置为“待审核”，如果不需要直接设置成“审核通过”。 8，如果需要审核就按之前说的异步推到消息队列。 架构图如下： 内容修改设计：流程和前面的创建差不多，只不过是修改的适合要考虑是否要送审。 如果修改之前已经达到送审的播放量，互动量的条件阈值，那么这一次修改需要送审： 1.有新增的图片，长文本，替换了新视频等等都是经过分布式对象存储系统。 2.填充字段。 3.查item_info中latest_version字段+1。 4.将元信息填到item_record表，设置item_record中的latest_status设置为”待审核“。 5.然后发给审核中心。 如果不需要审核： 前面基本一样，然后将元信息存到item_record表中，设置其中latest_status字段为”已上线“。 如果创作者没有改动内容本身，比只是调整了内容的可见性，则直接更新item_info表即可。 内容审核结果处理与版本控制设计：内容发布系统在收到event_audit_result消息队列的审核结果之后： 1.如果审核结果是通过，那么就将item_record表中的item_id，version的暂存数据覆盖到item_info表中去，表示内容发布了。 2.覆盖审核结果是不通过，那么就要设置item_record中latest_status为”审核拒绝“，并且填原因到latest_reason。 问题：假设我有用户短时间做了三次更新，编号分别是1，2，3，假设审核时间是1&gt;2&gt;3，那么就是最终的结果是1覆盖了所有，但是其实我们希望的是最后一次更新去覆盖，也就是说，我们要保证消息队列消息的顺序。 解决：很简单，因为版本号是递增的，所以我们去对比版本号就好了。 具体如下： 如果审核拒绝了，那么不进行下一步，如果通过了，如果online_version&#x3D;version,说明此次审核内容是线上内容可能达到阈值了，将结果写入item_info中的status字段。 如果online_version&lt;version说明审核结果版本比现实的新，就item_record替换item_info表数据就好了。 如果online_version&gt;version说明审核结果版本落后了，就继续下一步了。 流程图如下： 内容删除和下架设计：采用软删除，直接标记item_info表中的status为”被删除“或者”被下架“。 内容分发设计我们内容发布系统应该和内容分发渠道进行解耦，所以自然而然的会想到使用消息队列来解决。 如图： 内容展示设计","categories":[{"name":"项目学习","slug":"项目学习","permalink":"http://moonl0323.github.io/categories/%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/"},{"name":"后端开发","slug":"项目学习/后端开发","permalink":"http://moonl0323.github.io/categories/%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"后端开发","slug":"后端开发","permalink":"http://moonl0323.github.io/tags/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"},{"name":"架构设计","slug":"架构设计","permalink":"http://moonl0323.github.io/tags/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"}]},{"title":"用户关系服务业务架构说明","slug":"用户关系服务","date":"2024-11-30T16:00:00.000Z","updated":"2025-02-04T07:00:42.202Z","comments":true,"path":"2024/12/01/用户关系服务/","permalink":"http://moonl0323.github.io/2024/12/01/%E7%94%A8%E6%88%B7%E5%85%B3%E7%B3%BB%E6%9C%8D%E5%8A%A1/","excerpt":"","text":"用户关系服务业务架构说明 业务背景： 用户之间有互动交流，让用户可以更容易发现和关注其它用户的动态。 根据用户的关注可以做个性化推荐。 用户可以更容易发现自己和关注自己感兴趣的内容，留住用户。 服务接口职责 关注和取消关注接口：用户可以关注别的用户也可以取消对别的用户的关注。 查询用户关注列表接口：可以查看用户正在关注哪些其它的用户，按照最新的关注时间进行排序。 查询用户粉丝列表接口：可以查看用户有哪些正在关注他的人，按照最新的粉丝关注他的时间进行排序。 查询用户关注数和粉丝数接口：可以查看用户正在关注几个用户，有几个粉丝正在关注他。 查询用户之间关系的接口：比如可以查看用户1是否关注了用户2，用户2的粉丝是否包含了用户1等等。还有支持批量查询形式，比如查询用户1是否关注了指定的若干用户（批量查询关注关系），或者查询若干用户是否是用户1的粉丝（批量查询粉丝关系）。 基于Redis ZSET设计的具体操作与存在问题具体操作：数据结构选型：redis zset 维护对象： 一个是关注列表:Key设计为：following_{用户ID}，Member为被关注者的用户ID，对应的Score为关注行为发生的时间。 一个是粉丝列表:Key设计为：follower_{用户ID}，Member为粉丝用户ID，对应的Score为用户被关注行为发生的时间。 具体命令： 我们按照前面的五种接口来一一对应一下： 关注和取消关注接口：使用ZADD命令，如：用户1关注了用户2，具体命令如下： 12345//ZADD Key Score Member//操作关注列表ZADD following_用户1 时间戳 用户2//操作粉丝列表ZADD follower_用户2 时间戳 用户1 取消关注： 1234//ZREM Key Member//用户1取消关注用户2ZREM following_用户1 用户2ZREM follower_用户2 用户1 ​ 2.查询用户关注列表接口： 1234//ZREVRANGE Key start end 意思是查询该key列表的从start到end的所有member（从0开始），并且按照关注事件从近到远的排序//查询用户1的关注列表ZREVRANGE following_&#123;用户ID&#125; 0 -1//粉丝列表同理 ​ 3.同上 ​ 4.查询关注数和粉丝数 1//在redis中查询关注数就是查询关注列表的长度，也就是使用ZCARD命令 ​ 5.查询用户之间关系接口 ​ 5.1 单个查询：用户1是否关注了用户2，用户1是否为用户2的粉丝： 1234//用户1是否关注了用户2ZRANK following_用户1 用户2//用户1是否为用户2的粉丝ZRANK follower_用户2 用户1 ​ 5.2 批量查询：用户1是否关注了若干指定用户，若干指定用户是否为用户2粉丝： 12ZRANGE following_用户1 0 -1//先查出来用户1的关注列表如何看看指定用户在不在里面即可 存在问题：只用redis的话，由于redis是内存型数据库，那么在海量的用户量的时候，这个内存占用太多了，所有我们不考虑这种方案。 基于数据库设计单表设计​ User_relation 字段名 类型 含义 id BIGINT 自增主键，无特殊含义 from_user_id BIGINT 关注者用户ID to_user_id BIGINT 被关注者用户ID type INT 关注关系枚举 1表示正在关注，2表示取消关注 update_time DATE 记录修改时间 为了能够高效查询，我们需要建立索引。 索引建立考虑我们建立索引需要看具体的业务需要查询哪些数据，我们这里就是我们列的五种接口，除开第一个接口不是查询的，那么就是剩下四个查询接口，我们来一个一个看。 首先是查询用户关注列表，举个例子，要查用户1的关注列表，那么sql语句就是： 1SELECT to_user_id FROM User_relation WHERE from_user_id = 用户1 AND type = 1 ORDER BY update_time DESC; 显然我们需要为from_user_id建立索引。 同样的，我们列出其它几种接口的sql语句。 查询用户粉丝列表： 1SELECT from_user_id FROM User_relation WHERE to_user_id = 用户1 AND type = 1 ORDER BY update_time DESC; 查询用户之间关系： 用户1是否关注了用户2： 1SELECT 1 FROM User_relation WHERE from_user_id = 用户1 AND to_user_id = 用户2 AND type = 1; 用户2的粉丝是否包含了用户1： 1SELECT 1 FROM User_relation WHERE to_user_id = 用户2 AND from_user_id = 用户1 AND type = 1; 其它是同样的道理的，我们可以发现我们需要建立两种索引：1.from_user_id 2.to_user_id 那是不是我们只要建立这两种索引就够了，答案是否定的。 我们考虑大量用户量的情况。 假设拥有亿级的用户量，如果每个用户只是关注了1000个用户，那么我们的表都有千亿级别，显然不行，需要分库分表了。 我们考虑分表会发生什么情况。 我们考虑水平分表，那么我们一定是按照索引来分表的，为什么？ 我们先说说水平分表为什么要按照索引来分吧。 假设现在有一张表T(A,B,C); ​ T A B C a 1 a 3 b 2 b 4 c 6 现在我们设A是索引，我们考虑一下如果不按索引分表会发生什么，那么我们假设以B为标准划分表。 比如我们划分成三张表，1-2；3-4；&gt;4三种情况的表也就是： ​ T1 A B C a 1 b 2 ​ T2 A B C a 3 b 4 ​ T3 A B C c 6 那么考虑sql语句： 1SELECT * FROM T WHERE A=a 你看现在其实你需要去三张表里面都去找一遍。 但是如果你按照A索引分表，是不是就只要查一次就好了，因为大概率就是在某一张中。 那么回到我们实际的业务中呢，其实有两个索引，怎么办呢，答案是哪个作为标准分表都不合适。 比如你按照from_user_id索引进行分表： 1SELECT from_user_id FROM User_telation WHERE to_user_id = 用户1; 比如这条查询用户1的粉丝列表，那么就需要去n张子表都查一遍了。 反过来如果是查询用户1的关注列表，如果你按照to_user_id进行分表也是面临相同的问题。 那么该怎么办？ 我们试试竖直分表，但是字段本来就不多，那不如我们干脆拆开出来两张表，一张是following表，一张是follower表。 双表设计表结构如下： following表 字段名 类型 含义 id BIGINT 自增主键，无特殊含义 from_user_id BIGINT 关注者用户ID to_user_id BIGINT 被关注者用户ID type INT 关注关系枚举 1表示正在关注，2表示取消关注 update_time DATE 记录修改时间 follower表 字段名 类型 含义 id BIGINT 自增主键，无特殊含义 from_user_id BIGINT 关注者用户ID to_user_id BIGINT 被关注者用户ID type INT 关注关系枚举 1表示正在关注，2表示取消关注 update_time DATE 记录修改时间 你有没有发现他们其实和User_relation表是一模一样的，对的，就是一模一样的，我们刚刚分析问题是因为按照索引分表出现问题，那么我们这样拆开成两张表有个好处，我们两张表建立的索引不一样，他们负责的业务也不一样，这样子分表的时候就不会存在需要查大量子表的情况了。 比如，我们要查询用户的关注列表就可以使用following表，索引是from_user_id，这样分表后，由于只负责查询用户关注列表这一个业务动作，所以不会有任何问题，查询的数据大概率在同一张表之内，不会出现查询大量子表的情况。其它情况也是类似的。，比如查询用户的粉丝列表，那么查的就是follower表，索引是to_user_id，分表也不会出现问题。这几种接口都能命中索引，提高了查询的效率。 那么这种结构要求我们两张表的内容必须保持一致，也就是一张表必须是另一张表的copy，在创建和修改二者的数据时需要建立数据一致性关系。 这个好办，我们采用伪从技术，也就是follower表作为following表的伪从，消费following表产生的数据更新binlog。 架构图如下： binlog伪从技术我们采用阿里的Canal来做伪从技术，Canal的原理也就是Mysql DRC的原理，我们暂不展开，感兴趣的以后会写篇专门介绍。 最终选择Canal+Kafka来实现。 索引建立考虑我们先把前面那四种查询接口分类看看哪些需要Following表哪些是Follower表。 首先是查询用户关注列表, sql: 1SELECT to_user_id FROM T WHERE from_user_id = xxx AND type = 1 AND ORDER BY update_time; 显然建立from_user_id索引，那么也就是T是Following表。 我们看还需要type字段，同时还需要根据update_time进行排序，所以整个查询的过程应该是先查询from_user_id符合要求的，然后扫描type&#x3D;1的，最后对查询的用户还需要使用额外的空间进行排序，所以我们直接建立联合索引： 1KEY idx_following_list(from_user_id,type,update_time); 查询用户粉丝列表, sql: 1SELECT from_user_id FROM Follower WHERE to_user_id = xxx AND type = 1 AND ORDER BY update_time; 联合索引： 1KEY idx_follower_list(to_user_id,type,update_time); 查询用户1是否关注了用户2： sql： 1SELECT 1 FROM Following WHERE from_user_id = 用户1 AND to_user_id = 用户2 AND type = 1; 联合索引： 1KEY idx_following(from_user_id,to_user_id); 为什么不需要type字段做联合索引呢？因为我们是查到该记录就直接判断type是不是&#x3D;1就好了，而我们前面的查询关注列表，我们需要先根据from_user_id找到对应的记录，然后筛出type&#x3D;1的数据然后再进行排序，根本就是一个是SELECT 1只要判断是不是就行了，但是查询关注列表是SELECT to_user_id是一条条记录，所以需要type做联合索引。 查询用户2的粉丝是否包含用户1:sql： 1SELECT 1 FROM Follower WHERE to_user_id = 用户2 AND from_user_id = 用户1 AND type = 1; 联合索引： 1KEY idx_follower(to_user_id,from_user_id); 批量查询用户1是否关注了指定若干用户，比如是否关注了用户2，3，4： sql: 1SELECT to_user_id FROM Following WHERE from_user_id = 用户1 AND to_user_id IN (用户2，用户3，用户4); 联合索引： 1KEY idx_following(from_user_id,to_user_id); 同样的，批量查询用户2，3，4是否是用户1的粉丝： sql: 1SELECT to_user_id FROM Follower WHERE to_user_id =用户1 AND from_user_id IN (用户2，用户3，用户4); 联合索引： 1KEY idx_follower(to_user_id,from_user_id); 综上所述，每张表都各需要两个联合索引： Following表： 12KEY idx_following(from_user_id,to_user_id);KEY idx_following_list(from_user_id,type,update_time); Follower表： 12KEY idx_follower(to_user_id,from_user_id);KEY idx_follower_list(to_user_id,type,update_time); 虽然我们建立了多个联合索引占了空间，但是磁盘空间本身不值钱，所以这一波空间换时间其实是很划算的。 索引的回表问题及优化：我们前面不论是Following还是Follower表建立的索引都是非聚簇索引，也就是存储记录和索引分开了。 比如： 1SELECT 1 FROM Following WHERE from_user_id = 用户1 AND to_user_id = 用户2 AND type = 1; 这个走的索引是： 1KEY idx_following(from_user_id,to_user_id); 我们理清一下过程，首先走idx_following索引,查到对应的记录的主键了，接着走主键索引读它的type字段，这涉及了回表操作。 那么我们优化索引idx_following： 加多一个字段type： 1KEY idx_following(from_user_id,to_user_id,type); 同样的， idx_following_list查询用户的关注列表，首先查询联合索引，然后找到对应的主键id，回表查对应的to_user_id所以回表，改成; 1KEY idx_following_list(from_user_id,type,update_time,to_user_id); 同理，其它两种一样分析： idx_follower不需要改动： 因为我们要查的是： 1SELECT to_user_id FROM Follower WHERE to_user_id =用户1 AND from_user_id IN (用户2，用户3，用户4); 索引已经包含from_user_id了所以不用回表。 同样的改动idx_follower_list改成： 1KEY idx_follower_list(to_user_id,type,update_time,from_user_id); 缓存建立缓存数据选择我们还是看前面的接口来说明： 首先是获取关注列表，我们知道一般在社交软件来说，我们会限制用户关注的人数，比如只能关注200人，现实业务也不会有人一直频繁关注别人的，所以我们可以做个关注人数上限，那么也就是说关注列表是固定最大大小的，我们可以才去全量缓存。模型和我们前面基于Redis ZSET的方案一致。 然后是获取粉丝列表，显然关注列表人数有上限，但是一个用户的粉丝数量是无上限的，一些大v甚至能达到亿级的量。所以全量缓存是不现实的。我们从实际业务出发，我们会发现大v是不会去查看全部粉丝的，往往就是查询前几页，也就是上万的数量级，所以我们可以缓存10000条最近的粉丝列表。如果要查&gt;10000条外的就去查数据库就好了，但是这里会有安全问题，如果有人恶意攻击发送大量查询10000条外的粉丝的请求，那么数据库会崩，所以我们做限流，先查看当前请求是不是查询10000条之外的数据，如果是，就检查当前请求量是否已经达到限流阈值了，如果超过了直接拒绝执行，因为大概率是恶意攻击。 查询用户之间的关注关系，这个简单，比如查询用户1是否关注了用户2，用户3等，只要去查询关注列表里面有没有这些用户就好了。 麻烦的是如何批量查询一些用户是否是用户1的粉丝，如果用户1粉丝很少那还好，毕竟缓存命中率很高，万一很多怎么办，那么很大几率就是我们要查的一些用户它不在缓存里面。有一种解决方法就是反查关注列表，什么意思呢？举个例子，现在我们用户1有很多粉丝，存了最近的10000个在redis中的粉丝列表，现在我要查询100个用户是否是用户1的粉丝，我们发现只命中了10个，也就是剩下90个我不确定是不是用户1的粉丝，因为万一是10000个开外的呢。所以这个时候进行反查，我开90个线程让去查90个用户各自的关注列表里面有没有用户1，如果有那就是他的粉丝，没有就不是。问题很显然，万一很多，线程会爆掉，而且读请求被放大了。那还有什么方法呢？我们开多一个缓存去缓存粉丝关系，数据结构不是zset而是hash，其实Key是用户1，Field就是我要查的若干用户比如用户2，用户3，用户4等，Value就是1表示是粉丝，0表示不是粉丝。如果发现这个缓存里面有至少一个用户不在这个Hash中，那就得查数据库了。比如，我缓存了hash用户1，field是用户2，用户3。现在我查的其中有个用户4，我发现不在这里面，那么之前去查数据库。这个方案的问题就是redis空间会占的多，请求越多，这个缓存列表越大，所以要设置好过期时间。 缓存建立和更新策略创建缓存比较简单，就是我发出请求发现缓存有就查缓存，没有就查数据库，然后建立缓存。 那么更新策略呢？ 我们采取以往的，如果更新了，那么先更新数据库，然后删缓存，听起来没什么太大毛病。我们考虑极端情况。 因为大V来说经常有人会关注他的，如果和查看粉丝列表高并发执行的话。 比如交替进行： 有用户读取粉丝列表，发现没有缓存，于是去查数据库，然后建立缓存。 有新用户关注该大V，更新数据库，然后删除缓存。 又有用户读取粉丝列表，又重新建缓存。 。。。 我们会发现，很可能就是前脚刚建立缓存，下一秒就删掉了，这样子频繁的删除创建肯定不行的。所以粉丝列表绝对不能这么做。 那我们该怎么办呢？ 我们考虑一下，我们要的是缓存和数据库数据一致对吧，那么我们还是直接利用伪从技术，我们创建专门的消费者服务作为数据库Following表的伪从，这样子数据库Following表有变更，比如发生关注，取消关注事件了，消费者服务就会收到最新的数据变更binlog，然后去修改缓存。如果缓存存在就修改缓存。 我们现在考虑查询关注列表，由于在实际业务当中，没有人会一下子去关注很多人，所以关注列表比较稳定，所以之前先更新数据库删缓存的策略可以用，当然你也可以用伪从技术。 本地缓存我们在实际业务当中，往往会额外关注大V的关注列表，比如某某明星关注了谁，所以大V等公众人物的关注是很谨慎的，意味着他们的关注列表不会经常变化，但是读请求往往会很大，因为很多人好奇嘛，所以我们可以进一步建立关注列表的本地缓存，从而减少访问Redis，进一步提高访问性能。 普通用户的话关注了谁其实没有多少人关心，所以读请求不大，不需要本地缓存。 粉丝列表的话，大V的粉丝变动频繁，所以不适合本地缓存，普通用户的话，访问少，所以也不需要。 计数服务 缓存+数据库方案最终架构关注&#x2F;取消关注接口：直接更新数据库的Following表即可响应用户，过程异步。Follower表，计数服务，Redis缓存都会依赖Following产生的binlog表分别更新数据。 比如，用户1关注了用户2： 1.首先直接更新Following表的from_user_id,to_user_id,type和update_time字段。 2.Follower表会使用from_user_id(用户1)，to_user_id(用户2),关注时间。 3.计数服务会计数消费者请求计数服务增加用户1的关注数和用户2的粉丝数。 4.Redis缓存会缓存消费在用户1的关注列表缓存中加入用户2，在用户2的粉丝列表缓存中加入用户1. 查询用户关注列表接口： 先查本地缓存，没有就查Redis，还没有就查数据库，然后创建缓存。然后我们检查该用户的粉丝数是否达到一定阈值，如果是就是大V，我们将其关注列表加入本地缓存。 查询用户粉丝列表： 如果查的是最近10000个以内，那就直接查Redis，没有就查数据库，然后创建缓存。如果要查10000个开外的，就查看是不是达到请求阈值了，超过了就拒绝访问了，没超过就去查数据库。 查询用户关注数和粉丝数：直接去计数服务里面获取。 查询用户关系接口： 简单查询用户1是否关注了用户2：直接查看用户1的redis缓存的关注列表里面有没有用户2。如果换成不存在就查数据库，然后建立换成。 如果是批量查询用户1是否关注了若干用户：流程同上，只不过是获取到用户1的关注列表数据后，从中选出指定的用户。 查询若干用户是否是用户1的粉丝也就是查询若干用户是否关注了用户1： 如果用户1的粉丝数&lt;&#x3D;10000说明，粉丝列表是全量缓存的，直接查看粉丝列表缓存里面有没有这些用户，不用查数据库，因为没有在缓存里面说明就是不是粉丝。 如果用户1的粉丝超过了10000个，说明你查粉丝列表缓存没有，不代表就不是粉丝，因为缓存只存了10000个。所以只能用粉丝关系缓存hash了，我们查看hash缓存，看看这些用户在不在这里面，如果在直接看他们的值是不是1，是就代表这个用户是粉丝，0就表示该用户不是粉丝，直接返回结果。不需要查数据库。如果有用户不在hash中，我们就得回源数据库Follower表查询该用户和用户1的关系，将结果写到hash对象中去。 架构图如下： 拓展：加入图数据库NoSQL","categories":[{"name":"项目学习","slug":"项目学习","permalink":"http://moonl0323.github.io/categories/%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/"},{"name":"后端开发","slug":"项目学习/后端开发","permalink":"http://moonl0323.github.io/categories/%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"后端开发","slug":"后端开发","permalink":"http://moonl0323.github.io/tags/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"},{"name":"架构设计","slug":"架构设计","permalink":"http://moonl0323.github.io/tags/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"}]}],"categories":[{"name":"书籍阅读","slug":"书籍阅读","permalink":"http://moonl0323.github.io/categories/%E4%B9%A6%E7%B1%8D%E9%98%85%E8%AF%BB/"},{"name":"后端开发","slug":"书籍阅读/后端开发","permalink":"http://moonl0323.github.io/categories/%E4%B9%A6%E7%B1%8D%E9%98%85%E8%AF%BB/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"},{"name":"项目学习","slug":"项目学习","permalink":"http://moonl0323.github.io/categories/%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/"},{"name":"后端开发","slug":"项目学习/后端开发","permalink":"http://moonl0323.github.io/categories/%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"后端开发","slug":"后端开发","permalink":"http://moonl0323.github.io/tags/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"},{"name":"架构设计","slug":"架构设计","permalink":"http://moonl0323.github.io/tags/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"}]}